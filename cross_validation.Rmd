---
title: "<center><font aspan size = '6'>Cross Validation in R</font></center>"
author: "<center>Wyclife Agumba Oluoch wyclifeoluoch@gmail.com</center>"
date: "<center>`r Sys.time()`</center>"
output: 
  github_document: default
bibliography: packages.bib
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
packages <- c('base',
              'knitr',
              'rmarkdown',
              'purrr',
              'dplyr',
              'modelr')
installed_packages <- packages %in% rownames(installed.packages())
if(any(installed_packages == FALSE)){
  install.packages(packages[!installed_packages])
}
lapply(packages, library, character.only = TRUE) |> 
  invisible()
```

```{r write_bib, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
knitr::write_bib(c(
  .packages(), packages
), 'packages.bib') # Creates a bib file in the working directory
```

# Introduction

We will use mtcars dataset which comes with the base R. To get help about the dataset run ?mtcars in the console.

```{r}
head(mtcars) 
```


Breaking the dataset into five chuncks. This is the number of folds the data is being partitioned.  

```{r}
cv <- crossv_kfold(mtcars, k = 5)
cv
```



```{r}
models1 <- map(cv$train, ~lm(mpg ~ wt + cyl + hp, data = .))
models2 <- map(cv$train, ~lm(mpg ~ wt + qsec + am, data = .))
models3 <- map(cv$train, ~lm(mpg ~ wt + qsec + hp, data = .))
```

```{r}
get_pred <- function(model, test_data){
  data <- as.data.frame(test_data)
  pred <- add_predictions(data, model)
  return(pred)
}
pred1 <- map2_df(models1, cv$test, get_pred, .id = 'Run')
pred2 <- map2_df(models2, cv$test, get_pred, .id = 'Run')
pred3 <- map2_df(models3, cv$test, get_pred, .id = 'Run')
```


```{r}
MSE1 <- pred1 |> group_by(Run) |> summarise(MSE = mean((mpg - pred)^2))
MSE1
MSE2 <- pred2 |> group_by(Run) |> summarise(MSE = mean((mpg - pred)^2))
MSE1
MSE3 <- pred3 |> group_by(Run) |> summarise(MSE = mean((mpg - pred)^2))
MSE1
```

Here we then check for the mean square error.

```{r}
mean(MSE1$MSE)
mean(MSE2$MSE)
mean(MSE3$MSE)
```

The model with the lowest SME would be the best to use in making predictions over new data.

# References